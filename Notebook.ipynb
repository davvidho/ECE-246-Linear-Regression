{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section \\#1: Centralized Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section \\#1.1: LINEAR REGRESSION\n",
    "\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "from data_load import load\n",
    "import scipy.io as io\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from disk and perform preprocessing to prepare it for the linear regression problem.   \n",
    "    \"\"\"\n",
    "    X_train, y_train = load('regression_train.csv')\n",
    "    X_val, y_val = load('regression_val.csv')\n",
    "    X_test, y_test = load('regression_test.csv')\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test= get_data()  \n",
    "\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Validation data shape: ',X_val.shape)\n",
    "print('Validation target shape: ',y_val.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)\n",
    "#take note of all the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the training and test data ##\n",
    "\n",
    "plt.plot(X_train, y_train,'o', color='black')\n",
    "plt.plot(X_test, y_test,'o', color='blue')\n",
    "plt.xlabel('Train data')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Linear Regression\n",
    "\n",
    "In the following cells, you will build a linear regression. You will implement its loss function, then subsequently train it with gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the opimal solution using closed form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Regression import Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function and gradient are calculated with the initial weight vector set to zero\n",
    "regression = Regression(m=1, reg_param=0)\n",
    "loss, grad = regression.loss_and_grad(X_train,y_train)\n",
    "print(loss)\n",
    "print(grad)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the data with gradient descent, feel free to play around with learning rate, batch size, and iterations\n",
    "loss_history, w = regression.train_LR(X_train,y_train, eta=1e-3,batch_size=20, num_iters=10000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "print(w)\n",
    "print(loss_history[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is different from before because this is an analytical solution, aka equate the gradient to zero and solve for w\n",
    "loss_2, w_2 = regression.closed_form(X_train, y_train)\n",
    "print(loss_2)\n",
    "print(w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "#polynomial hypothesis complexity: aka x, x^2, x^3 etc for m = 1, 2, and 3\n",
    "#does a more complex hypothesis mean that you will get better results?\n",
    "N,d = X_test.shape\n",
    "for i in range(10):\n",
    "    regression_train = Regression(m=i+1, reg_param=0)\n",
    "    loss_2, w_2 = regression_train.closed_form(X_train, y_train)\n",
    "    X_bias = regression_train.gen_poly_features(X_test)\n",
    "    print(w_2)\n",
    "    train_loss[i]= loss_2\n",
    "    test_loss[i]= (1/N)*np.sum(np.square(np.reshape(y_test, (N,d))-np.dot(X_bias, w_2)))\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Loss')\n",
    "x_axis = np.linspace(1,10, num = 10)\n",
    "plt.title('Loss Function with respect to polynomial degree')\n",
    "plt.plot(x_axis, train_loss,  color='black')\n",
    "plt.plot(x_axis, test_loss, color='blue')\n",
    "plt.show()\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# plot both the training and test loss in the same plot\n",
    "# for m = 10, vary the regularization term and see how this affects the loss\n",
    "N,d = X_test.shape\n",
    "for i in range(10):\n",
    "    if(i==0):\n",
    "        regression_train = Regression(m=10, reg_param=0)\n",
    "        loss_2, w_2 = regression_train.closed_form(X_train, y_train)\n",
    "        train_loss[i]= loss_2\n",
    "        X_bias = regression_train.gen_poly_features(X_test)\n",
    "        test_loss[i]= (1/N)*np.sum(np.square(np.reshape(y_test, (N,d))-np.dot(X_bias, w_2)))\n",
    "        print(w_2)\n",
    "    else:\n",
    "        regression_train = Regression(m=10, reg_param = 10**(-9+i))\n",
    "        lamda = 10**(-9+i)\n",
    "        loss_2, w_2 = regression_train.closed_form(X_train, y_train)\n",
    "        train_loss[i]= loss_2\n",
    "        print(w_2)\n",
    "        X_bias = regression_train.gen_poly_features(X_test)\n",
    "        test_loss[i] = (1/N)*np.sum(np.square(np.reshape(y_test,(N,d))-np.dot(X_bias, w_2))) + (0.5*lamda*np.sum(np.square(w_2)))\n",
    "plt.xlabel('Log Scaled Regularization Factor ')\n",
    "plt.ylabel('Loss')\n",
    "x_axis = np.linspace(1,10, num = 10)\n",
    "plt.title('Loss as a function of regularization parameter')\n",
    "plt.plot(x_axis, train_loss,  color='black')\n",
    "plt.plot(x_axis, test_loss, color='blue')   \n",
    "print(train_loss)\n",
    "print(test_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
